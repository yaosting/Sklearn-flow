{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 当我们拿到一批原始的数据\n",
    "\n",
    "# 1.首先要明确有多少特征，哪些是连续的，哪些是类别的。\n",
    "# 2.检查有没有缺失值，对确实的特征选择恰当方式进行弥补，使数据完整。\n",
    "# 3.对连续的数值型特征进行标准化，使得均值为0，方差为1。\n",
    "# 4.对类别型的特征进行one-hot编码。\n",
    "# 5.将需要转换成类别型数据的连续型数据进行二值化。\n",
    "# 6.为防止过拟合或者其他原因，选择是否要将数据进行正则化。\n",
    "# 7.在对数据进行初探之后发现效果不佳，可以尝试使用多项式方法，寻找非线性的关系。\n",
    "# 8.根据实际问题分析是否需要对特征进行相应的函数转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (150,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载数据\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "#新版的库将cross_validation 改为model_selection\n",
    "# from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "iris  = datasets.load_iris()\n",
    "type(iris.data),iris.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.56195149, -1.22474487,  1.33630621],\n",
       "        [ 1.40487872,  0.        , -0.26726124],\n",
       "        [-0.84292723,  1.22474487, -1.06904497]]), numpy.ndarray)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预处理数据的方法总结（使用sklearn-preprocessing）\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# 1. 标准化：去均值，方差规模化\n",
    " \n",
    "# 创建一组特征数据，每一行表示一个样本，每一列表示一个特征\n",
    "# Standardization标准化:将特征数据的分布调整成标准正太分布，也叫高斯分布，也就是使得数据的均值维0，方差为1.\n",
    "# 标准化的原因在于如果有些特征的方差过大，则会主导目标函数从而使参数估计器无法正确地去学习其他特征。\n",
    "# 标准化的过程为两步：去均值的中心化（均值变为0）；方差的规模化（方差变为1）。\n",
    "# 在sklearn.preprocessing中提供了一个scale的方法，可以实现以上功能。\n",
    "x = np.array([[1., -1., 2.],\n",
    "              [8., 0., 0.],\n",
    "              [0., 1., -1.]])\n",
    "# 将每一列特征标准化为标准正太分布，注意，标准化是针对每一列而言的\n",
    "x_scale = preprocessing.scale(x)\n",
    "x_scale,type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以查看标准化后的数据的均值与方差，已经变成0,1了\n",
    "# axis=0 表示对每一列\n",
    "x_scale.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同理，看一下标准差\n",
    "x_scale.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing这个模块还提供了一个实用类StandarScaler，它可以在训练数据集上做了标准转换操作之后，把相同的转换应用到测试训练集中。\n",
    "# 这是相当好的一个功能。可以对训练数据，测试数据应用相同的转换，以后有新的数据进来也可以直接调用，不用再重新把数据放在一起再计算一次了。\n",
    "# 调用fit方法，根据已有的训练数据创建一个标准化的转换器\n",
    "# 另外，StandardScaler()中可以传入两个参数：with_mean,with_std.这两个都是布尔型的参数，\n",
    "# 默认情况下都是true,但也可以自定义成false.即不要均值中心化或者不要方差规模化为1.\n",
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    " \n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12390297,  1.22474487, -0.26726124]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用上面这个转换器去转换训练数据x,调用transform方法\n",
    "old  =scaler.transform(x)\n",
    "########################################\n",
    "# 好了，比如现在又来了一组新的样本，也想得到相同的转换\n",
    "new_x = [[-1., 1., 0.]]\n",
    "new  =scaler.transform(new_x)\n",
    "new\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.12390297,  1.22474487, -0.26726124])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old.mean(axis= 0)#  对测试集进行相同的标准化操作后 不一定会获得均值为一的数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.375     ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.MinMaxScaler\n",
    "# 在MinMaxScaler中是给定了一个明确的最大值与最小值。它的计算公式如下：\n",
    "# X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "# X_scaled = X_std / (max - min) + min\n",
    "# 以下这个例子是将数据规与[0,1]之间，每个特征中的最小值变成了0，最大值变成了1，请看：\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_minmax = min_max_scaler.fit_transform(x)\n",
    "x_minmax\n",
    "################################################################################\n",
    "# 同样的，如果有新的测试数据进来，也想做同样的转换咋办呢？请看：\n",
    "x_test = np.array([[-3., -1., 4.]])\n",
    "x_test_minmax = min_max_scaler.transform(x_test)\n",
    "x_test_minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.125     ,  1.5       ,  1.33333333]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new  =np.array( [[1.,2.,3.]])\n",
    "new = min_max_scaler.transform(new)\n",
    "new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer(copy=True, norm='l2') 111\n",
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]] 222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678,  0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#以上都是对数据进行标准化 中心化 通过构造一个scaler来进行统一操作\n",
    "#3.同理构造一个正则器来进行正则\n",
    "# preprocessing这个模块还提供了一个实用类Normalizer,实用transform方法同样也可以对新的数据进行同样的转换\n",
    "# 根据训练数据创建一个正则器\n",
    "normalizer = preprocessing.Normalizer().fit(x)\n",
    "print(normalizer,111)\n",
    "# 对训练数据进行正则\n",
    "print(normalizer.transform(x),222)\n",
    "###################################################################\n",
    "# 对新的测试数据进行正则\n",
    "normalizer.transform([[-1., 1., 0.]])\n",
    "# normalize和Normalizer都既可以用在密集数组也可以用在稀疏矩阵（scipy.sparse)中\n",
    "# 对于稀疏的输入数据，它会被转变成维亚索的稀疏行表征（具体请见scipy.sparse.csr_matrix)\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.二值化\n",
    "# 3 二值化–特征的二值化\n",
    "# 特征的二值化是指将数值型的特征数据转换成布尔类型的值。可以使用实用类Binarizer\n",
    "# 默认是根据0来二值化，大于0的都标记为1，小于等于0的都标记为0。\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    " \n",
    "# 创建一组特征数据，每一行表示一个样本，每一列表示一个特征\n",
    "x = np.array([[1., -1., 2.],\n",
    "              [2., 0., 0.],\n",
    "              [0., 1., -1.]])\n",
    " \n",
    "binarizer = preprocessing.Binarizer().fit(x)\n",
    "binarizer.transform(x)\n",
    "#################################################################################\n",
    "#当然也可以自己设置这个阀值，超过多少认为是1，只需传出参数threshold即可\n",
    "binarizer = preprocessing.Binarizer(threshold=1.5)\n",
    "binarizer.transform(x)\n",
    "##########################################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.标签编码\n",
    "import sklearn\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "le.fit([1, 2, 2, 6])\n",
    "le.transform([1, 1, 2, 6]) #array([0, 0, 1, 2])\n",
    "#非数值型转化为数值型\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "le.transform([\"tokyo\", \"tokyo\", \"paris\"]) #array([2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6..OneHotEncoder独热编码\n",
    "#独热编码和标签编码 前者都是用01来表示，并且任意两个变量在向量空间距离都相等。标签编码则用1~n来表示n个离散值，这样每个特征会不平等\n",
    "#可以结合二者  先将数据标签化 再独热\n",
    "from sklearn import preprocessing\n",
    "enc = preprocessing.OneHotEncoder(categories='auto')\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])    # fit来学习编码\n",
    "enc.transform([[0, 1, 3]]).toarray()    # 进行编码\n",
    "#fit进去的有4*3的数据，特征有3为，第一维有 0 1 两种值所以独热为\n",
    "#独热编码（哑变量 dummy variable）是因为大部分算法是基于向量空间中的度量来进行计算的，为了使非偏序关系的变量取值不具有偏序性，\n",
    "#并且到圆点是等距的。使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。\n",
    "#将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。\n",
    "#离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征。\n",
    "#就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SBY\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:331: DeprecationWarning: Passing 'n_values' is deprecated in version 0.20 and will be removed in 0.22. You can use the 'categories' keyword instead. 'n_values=n' corresponds to 'categories=[range(n)]'.\n",
      "  warnings.warn(msg, DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.]), array([ 0.,  1.]), array([ 0.,  1.,  2.])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为类别特征编码\n",
    "# 我们知道特征可能是连续型的也可能是类别型的变量，比如说：\n",
    "# [“male”, “female”], [“from Europe”, “from US”, “from Asia”], [“uses Firefox”, “uses Chrome”, “uses Safari”, “uses Internet Explorer”].\n",
    "# 这些类别特征无法直接进入模型，它们需要被转换成整数来表征，比如：\n",
    "# [“male”, “from US”, “uses Internet Explorer”] could be expressed as [0, 1, 3] while [“female”, “from Asia”, “uses Chrome”] would be [1, 2, 1].\n",
    "# 然而上面这种表征的方式仍然不能直接为scikit-learn的模型所用，因为模型会把它们当成序列型的连续变量。\n",
    "# 要想使得类别型的变量能最终被模型直接使用，可以使用one-of-k编码或者one-hot编码。\n",
    "# 这些都可以通过OneHotEncoder实现，它可以将有n种值的一个特征变成n个二元的特征。\n",
    "# 特征1中有(0,1）两个值，特征2中有(0,1,2)3个值，特征3中有（0,1,2,3)4个值，所以编码之后总共有9个二元特征。\n",
    " \n",
    "enc = preprocessing.OneHotEncoder(categories='auto')#categories='auto'用于设置自动寻找该列的特征种类\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "enc.transform([[0,1,3]]).toarray()\n",
    "##########################################################################\n",
    "# 但是呢，也会存在这样的情况，某些特征中可能对一些值有缺失，\n",
    "# 比如明明有男女两个性别，样本数据中都是男性，这样就会默认被判别为我只有一类值。\n",
    "# 这个时候我们可以向OneHotEncoder传如参数n_values，用来指明每个特征中的值的总个数\n",
    "enc = preprocessing.OneHotEncoder(n_values=[2,3,4])  # 指明每个特征中的值的总个数分别为 2 3 4\n",
    "enc.fit([[1, 2, 3], [0, 2, 0]])\n",
    "enc.transform([[1,0,0]]).toarray()\n",
    "enc.categories_\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#缺点：当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用PCA来减少维度。\n",
    "#而且one hot encoding+PCA这种组合在实际中也非常有用。\n",
    "\n",
    "#将离散数据进行独热编码是为了更好计算向量距离，如果模型本身对距离不要求或者离散数据过多，不适合使用+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.          2.        ]\n",
      " [ 6.          9.66666667]\n",
      " [ 7.          6.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SBY\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\SBY\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 7.. 弥补缺失数据\n",
    "# 在scikit-learn的模型中都是假设输入的数据是数值型的，并且都是有意义的，如果有缺失数据是通过NAN，或者空值表示的话，就无法识别与计算了。\n",
    "# 要弥补缺失值，可以使用均值，中位数，众数等等。Imputer这个类可以实现。请看：\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "#from sklearn import impute.SimpleImputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit([[1, 20], [np.nan, 3], [7, 6]])\n",
    "Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SBY\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4.        ,  2.        ],\n",
       "       [ 6.        ,  3.66666667],\n",
       "       [ 7.        ,  6.        ]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputer类同样也可以支持稀疏矩阵,以下例子将0作为了缺失值，为其补上均值\n",
    "import scipy.sparse as sp\n",
    " \n",
    "# 创建一个稀疏矩阵\n",
    "x = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])\n",
    " \n",
    "imp = Imputer(missing_values=0, strategy='mean', verbose=0)\n",
    "imp.fit(x)\n",
    " \n",
    "x_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])\n",
    "imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.,   0.,   0.,   1.],\n",
       "       [  1.,   2.,   3.,   4.,   6.,   9.],\n",
       "       [  1.,   4.,   5.,  16.,  20.,  25.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.. 创建多项式特征\n",
    "# 有的时候线性的特征并不能做出美的模型，于是我们会去尝试非线性。非线性是建立在将特征进行多项式地展开上的。\n",
    " \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    " \n",
    "# 自建一组3*2的样本\n",
    "x = np.arange(6).reshape(3, 2)\n",
    "# 创建2次方的多项式\n",
    "# 比如将两个特征 (X_1, X_2)，它的平方展开式便转换成5个特征(1, X_1, X_2, X_1^2, X_1X_2, X_2^2). 代码案例如下：\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],\n",
       "       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],\n",
       "       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以自定义选择只要保留特征相乘的项。 \n",
    "# 即将 (X_1, X_2, X_3) 转换成 (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3).\n",
    "x = np.arange(9).reshape(3, 3)\n",
    " \n",
    "poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "poly.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#9. 去除异常数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.69314718],\n",
       "       [ 1.09861229,  1.38629436]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. 自定义转换器\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "    \n",
    "transformer = FunctionTransformer(np.log1p,validate=False)\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "transformer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
